Creating sequence-to-sequences 
with attention mechanism 

2016-08-08 12:09:22.790632
 3 layers of 1024 units  2 context size 128 bach-size. 
 128 batch size 2542 number of steps to complete one epoch 
global step 200 learning rate 0.0010 step-time 0.22 loss 1.68  perplexity 5.39 
global step 400 learning rate 0.0010 step-time 0.22 loss 1.39  perplexity 4.02 
global step 600 learning rate 0.0010 step-time 0.22 loss 1.28  perplexity 3.59 
global step 800 learning rate 0.0010 step-time 0.22 loss 1.20  perplexity 3.33 
global step 1000 learning rate 0.0010 step-time 0.22 loss 1.19  perplexity 3.28 
global step 1200 learning rate 0.0010 step-time 0.22 loss 1.17  perplexity 3.23 
global step 1400 learning rate 0.0010 step-time 0.22 loss 1.16  perplexity 3.18 
global step 1600 learning rate 0.0010 step-time 0.22 loss 1.15  perplexity 3.16 
global step 1800 learning rate 0.0010 step-time 0.22 loss 1.16  perplexity 3.18 
global step 2000 learning rate 0.0010 step-time 0.22 loss 1.15  perplexity 3.17 
global step 2200 learning rate 0.0010 step-time 0.22 loss 1.15  perplexity 3.14 
global step 2400 learning rate 0.0010 step-time 0.22 loss 1.13  perplexity 3.10 
epoch  0 finished 
  train:  loss 1.1322 perplexity 3.1024 
  eval:  loss 1.1457 perplexity 3.1446 
global step 2600 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.08 
global step 2800 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.08 
global step 3000 learning rate 0.0010 step-time 0.22 loss 1.14  perplexity 3.12 
global step 3200 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.05 
global step 3400 learning rate 0.0010 step-time 0.22 loss 1.13  perplexity 3.09 
global step 3600 learning rate 0.0010 step-time 0.22 loss 1.14  perplexity 3.11 
global step 3800 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 4000 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.08 
global step 4200 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.06 
global step 4400 learning rate 0.0010 step-time 0.22 loss 1.13  perplexity 3.08 
global step 4600 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.08 
global step 4800 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.05 
global step 5000 learning rate 0.0010 step-time 0.22 loss 1.13  perplexity 3.09 
epoch  1 finished 
  train:  loss 1.1154 perplexity 3.0509 
  eval:  loss 1.1371 perplexity 3.1177 
global step 5200 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.06 
global step 5400 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.08 
global step 5600 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 5800 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.06 
global step 6000 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.02 
global step 6200 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.05 
global step 6400 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.05 
global step 6600 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.07 
global step 6800 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.07 
global step 7000 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 7200 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 7400 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 7600 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.07 
epoch  2 finished 
  train:  loss 1.1096 perplexity 3.0333 
  eval:  loss 1.1318 perplexity 3.1011 
Creating sequence-to-sequences 
with attention mechanism 
Continue training existing model! 
2016-08-08 15:03:11.713004
 3 layers of 1024 units  2 context size 128 bach-size. 
 128 batch size 2542 number of steps to complete one epoch 
global step 5184 learning rate 0.0010 step-time 0.23 loss 1.12  perplexity 3.06 
global step 5284 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 5384 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 5484 learning rate 0.0010 step-time 0.22 loss 1.13  perplexity 3.10 
global step 5584 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 5684 learning rate 0.0010 step-time 0.22 loss 1.13  perplexity 3.10 
global step 5784 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 5884 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.05 
global step 5984 learning rate 0.0010 step-time 0.22 loss 1.13  perplexity 3.09 
global step 6084 learning rate 0.0010 step-time 0.22 loss 1.13  perplexity 3.08 
global step 6184 learning rate 0.0010 step-time 0.22 loss 1.13  perplexity 3.11 
global step 6284 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 6384 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 6484 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 6584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.02 
global step 6684 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.05 
global step 6784 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 6884 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 6984 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.07 
global step 7084 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 7184 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 7284 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 7384 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 7484 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 7584 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
epoch  2 finished 
  train:  loss 1.1142 perplexity 3.0471 
  eval:  loss 1.1369 perplexity 3.1171 
global step 7684 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 7784 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.05 
global step 7884 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 7984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 8084 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 8184 learning rate 0.0010 step-time 0.22 loss 1.13  perplexity 3.08 
global step 8284 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 8384 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 8484 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.08 
global step 8584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 8684 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.06 
global step 8784 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 8884 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 8984 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.05 
global step 9084 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 9184 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.05 
global step 9284 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 9384 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 9484 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 9584 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.06 
global step 9684 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 9784 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 9884 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 9984 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 10084 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
epoch  3 finished 
  train:  loss 1.1071 perplexity 3.0256 
  eval:  loss 1.1337 perplexity 3.1072 
global step 10184 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.05 
global step 10284 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 10384 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 10484 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 10584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 10684 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 10784 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 10884 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.05 
global step 10984 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 11084 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 11184 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 11284 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 11384 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.05 
global step 11484 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.05 
global step 11584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.02 
global step 11684 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 11784 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 11884 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 11984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 12084 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.05 
global step 12184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 12284 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 12384 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 12484 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 12584 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 12684 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
epoch  4 finished 
  train:  loss 1.1017 perplexity 3.0092 
  eval:  loss 1.1311 perplexity 3.0992 
global step 12784 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 12884 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 12984 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 13084 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 13184 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 13284 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 13384 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 13484 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.06 
global step 13584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 13684 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.02 
global step 13784 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 13884 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 13984 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 14084 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 14184 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.06 
global step 14284 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 14384 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 14484 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 14584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 14684 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 14784 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.96 
global step 14884 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 14984 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 15084 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 15184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
epoch  5 finished 
  train:  loss 1.1034 perplexity 3.0143 
  eval:  loss 1.1329 perplexity 3.1045 
global step 15284 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 15384 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 15484 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 15584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.02 
global step 15684 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 15784 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 15884 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 15984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 16084 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 16184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 16284 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.05 
global step 16384 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 16484 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.02 
global step 16584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 16684 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 16784 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 16884 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 16984 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 17084 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 17184 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 17284 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 17384 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 17484 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.02 
global step 17584 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 17684 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 17784 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
epoch  6 finished 
  train:  loss 1.0998 perplexity 3.0035 
  eval:  loss 1.1303 perplexity 3.0966 
global step 17884 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.95 
global step 17984 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 18084 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 18184 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 18284 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 18384 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.96 
global step 18484 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 18584 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 18684 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 18784 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 18884 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 18984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 19084 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 19184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 19284 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 19384 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 19484 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 19584 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 19684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 19784 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 19884 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 19984 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 20084 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 20184 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 20284 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
epoch  7 finished 
  train:  loss 1.0963 perplexity 2.9931 
  eval:  loss 1.1290 perplexity 3.0926 
global step 20384 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 20484 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.96 
global step 20584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 20684 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 20784 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.02 
global step 20884 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 20984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 21084 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 21184 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.95 
global step 21284 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 21384 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 21484 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 21584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 21684 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.95 
global step 21784 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.95 
global step 21884 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 21984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 22084 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 22184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 22284 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.95 
global step 22384 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 22484 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 22584 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.95 
global step 22684 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 22784 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
epoch  8 finished 
  train:  loss 1.0991 perplexity 3.0016 
  eval:  loss 1.1307 perplexity 3.0979 
global step 22884 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 22984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.02 
global step 23084 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.95 
global step 23184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 23284 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 23384 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 23484 learning rate 0.0010 step-time 0.22 loss 1.07  perplexity 2.92 
global step 23584 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 23684 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 23784 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 23884 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.95 
global step 23984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 24084 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 24184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 24284 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 24384 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 24484 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 24584 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 24684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 24784 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 24884 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 24984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 25084 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 25184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 25284 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 25384 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.02 
epoch  9 finished 
  train:  loss 1.0936 perplexity 2.9850 
  eval:  loss 1.1266 perplexity 3.0851 
global step 25484 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 25584 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 25684 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 25784 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 25884 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 25984 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.95 
global step 26084 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 26184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 26284 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.96 
global step 26384 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 26484 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 26584 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 26684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 26784 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 26884 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 26984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 27084 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 27184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 27284 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.96 
global step 27384 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 27484 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.96 
global step 27584 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 27684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 27784 learning rate 0.0010 step-time 0.22 loss 1.13  perplexity 3.08 
global step 27884 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
epoch  10 finished 
  train:  loss 1.0957 perplexity 2.9912 
  eval:  loss 1.1272 perplexity 3.0871 
global step 27984 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 28084 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 28184 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.94 
global step 28284 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 28384 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.02 
global step 28484 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 28584 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.94 
global step 28684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.96 
global step 28784 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 28884 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.95 
global step 28984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 29084 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 29184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.96 
global step 29284 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 29384 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 29484 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 29584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 29684 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 29784 learning rate 0.0010 step-time 0.22 loss 1.07  perplexity 2.92 
global step 29884 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 29984 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 30084 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.02 
global step 30184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 30284 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 30384 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.96 
global step 30484 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
epoch  11 finished 
  train:  loss 1.0935 perplexity 2.9848 
  eval:  loss 1.1282 perplexity 3.0902 
global step 30584 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 30684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 30784 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.96 
global step 30884 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 30984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 31084 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.94 
global step 31184 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 31284 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 31384 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 31484 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 31584 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 31684 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.96 
global step 31784 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 31884 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 31984 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.94 
global step 32084 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 32184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 32284 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 32384 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.96 
global step 32484 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 32584 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 32684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 32784 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 32884 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 32984 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
epoch  12 finished 
  train:  loss 1.0941 perplexity 2.9865 
  eval:  loss 1.1272 perplexity 3.0871 
global step 33084 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 33184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 33284 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.02 
global step 33384 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 33484 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 33584 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 33684 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 33784 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 33884 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 33984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 34084 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 34184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 34284 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 34384 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 34484 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 34584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 34684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 34784 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 34884 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.94 
global step 34984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 35084 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 35184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 35284 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 35384 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 35484 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 35584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
epoch  13 finished 
  train:  loss 1.0950 perplexity 2.9893 
  eval:  loss 1.1289 perplexity 3.0921 
global step 35684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 35784 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 35884 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 35984 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 36084 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 36184 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.94 
global step 36284 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 36384 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.96 
global step 36484 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.96 
global step 36584 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 36684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 36784 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 36884 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 36984 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 37084 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 37184 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 37284 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 37384 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 37484 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 37584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 37684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 37784 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 37884 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 37984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 38084 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
epoch  14 finished 
  train:  loss 1.0964 perplexity 2.9934 
  eval:  loss 1.1286 perplexity 3.0913 
global step 38184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 38284 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.95 
global step 38384 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.95 
global step 38484 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 38584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 38684 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 38784 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 38884 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.95 
global step 38984 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 39084 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 39184 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 39284 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 39384 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 39484 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 39584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 39684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.96 
global step 39784 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 39884 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 39984 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.96 
global step 40084 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.96 
global step 40184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 40284 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 40384 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.06 
global step 40484 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 40584 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
epoch  15 finished 
  train:  loss 1.0960 perplexity 2.9922 
  eval:  loss 1.1291 perplexity 3.0927 
global step 40684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 40784 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 40884 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 40984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.02 
global step 41084 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 41184 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 41284 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 41384 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 41484 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 41584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 41684 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 41784 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 41884 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 41984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 42084 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 42184 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.96 
global step 42284 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 42384 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 42484 learning rate 0.0010 step-time 0.22 loss 1.12  perplexity 3.06 
global step 42584 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 42684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 42784 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.96 
global step 42884 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 42984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.02 
global step 43084 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.94 
global step 43184 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
epoch  16 finished 
  train:  loss 1.0948 perplexity 2.9886 
  eval:  loss 1.1295 perplexity 3.0941 
global step 43284 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 43384 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 43484 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.96 
global step 43584 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 43684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 43784 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 43884 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 43984 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.96 
global step 44084 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 44184 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 44284 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.95 
global step 44384 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 44484 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 44584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 44684 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 44784 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 44884 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 44984 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.96 
global step 45084 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 45184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 45284 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 45384 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 45484 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 45584 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 45684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
epoch  17 finished 
  train:  loss 1.0943 perplexity 2.9870 
  eval:  loss 1.1299 perplexity 3.0954 
global step 45784 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.02 
global step 45884 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 45984 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 46084 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 46184 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 46284 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 46384 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 46484 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 46584 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 46684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 46784 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 46884 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 46984 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 47084 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 47184 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.04 
global step 47284 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 47384 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.96 
global step 47484 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 47584 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 47684 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 47784 learning rate 0.0010 step-time 0.22 loss 1.08  perplexity 2.95 
global step 47884 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.02 
global step 47984 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 48084 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 48184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 48284 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
epoch  18 finished 
  train:  loss 1.0929 perplexity 2.9830 
  eval:  loss 1.1285 perplexity 3.0909 
global step 48384 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 48484 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 48584 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 48684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 48784 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 48884 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 48984 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 49084 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.98 
global step 49184 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 49284 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.96 
global step 49384 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 49484 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 49584 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.96 
global step 49684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 49784 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 49884 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.02 
global step 49984 learning rate 0.0010 step-time 0.22 loss 1.11  perplexity 3.03 
global step 50084 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
global step 50184 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 50284 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 50384 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.99 
global step 50484 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.01 
global step 50584 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 3.00 
global step 50684 learning rate 0.0010 step-time 0.22 loss 1.09  perplexity 2.97 
global step 50784 learning rate 0.0010 step-time 0.22 loss 1.10  perplexity 2.99 
epoch  19 finished 
  train:  loss 1.0954 perplexity 2.9905 
  eval:  loss 1.1312 perplexity 3.0994 
Reached the maximum number of epochs 20  stop trainig 
best training loss 2.9830 best validation 3.0851 
  test:  loss 1.1277 perplexity 3.0885 
