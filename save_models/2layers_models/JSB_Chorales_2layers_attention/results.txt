
2016-08-03 20:39:32.013726
Creating sequence-to-sequences 
with attention mechanism 
 2 layers of 1024 units  2 context size 128 bach-size. 
 128 batch size 2542 number of steps to complete one epoch 
global step 100 learning rate 0.0010 step-time 0.09 loss 1.76  perplexity 5.82 
global step 200 learning rate 0.0010 step-time 0.09 loss 1.40  perplexity 4.08 
global step 300 learning rate 0.0010 step-time 0.09 loss 1.32  perplexity 3.73 
global step 400 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.45 
global step 500 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.39 
global step 600 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.26 
global step 700 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.25 
global step 800 learning rate 0.0010 step-time 0.09 loss 1.16  perplexity 3.19 
global step 900 learning rate 0.0010 step-time 0.09 loss 1.16  perplexity 3.20 
global step 1000 learning rate 0.0010 step-time 0.09 loss 1.16  perplexity 3.19 
global step 1100 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.25 
global step 1200 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.24 
global step 1300 learning rate 0.0010 step-time 0.09 loss 1.15  perplexity 3.16 
global step 1400 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.14 
global step 1500 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.13 
global step 1600 learning rate 0.0010 step-time 0.09 loss 1.16  perplexity 3.17 
global step 1700 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.13 
global step 1800 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.08 
global step 1900 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.11 
global step 2000 learning rate 0.0010 step-time 0.09 loss 1.15  perplexity 3.15 
global step 2100 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.12 
global step 2200 learning rate 0.0010 step-time 0.09 loss 1.15  perplexity 3.16 
global step 2300 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.09 
global step 2400 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.12 
global step 2500 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.09 
epoch  0 finished 
  avg train batch:  loss 1.1974 perplexity 3.3116 
  eval:  loss 1.1435 perplexity 3.1489 
global step 2600 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.13 
global step 2700 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.12 
global step 2800 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.12 
global step 2900 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.14 
global step 3000 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.08 
global step 3100 learning rate 0.0010 step-time 0.09 loss 1.15  perplexity 3.14 
global step 3200 learning rate 0.0010 step-time 0.09 loss 1.15  perplexity 3.15 
global step 3300 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.11 
global step 3400 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.10 
global step 3500 learning rate 0.0010 step-time 0.09 loss 1.11  perplexity 3.03 
global step 3600 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.10 
global step 3700 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.06 
global step 3800 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.09 
global step 3900 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.08 
global step 4000 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.12 
global step 4100 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.09 
global step 4200 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.08 
global step 4300 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.08 
global step 4400 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.10 
global step 4500 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.09 
global step 4600 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.05 
global step 4700 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
global step 4800 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.10 
global step 4900 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.11 
global step 5000 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.09 
epoch  1 finished 
  avg train batch:  loss 1.1303 perplexity 3.0966 
  eval:  loss 1.1423 perplexity 3.1444 
global step 5100 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
global step 5200 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
global step 5300 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
global step 5400 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.08 
global step 5500 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.05 
global step 5600 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.08 
global step 5700 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.10 
global step 5800 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.10 
global step 5900 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.06 
global step 6000 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.09 
global step 6100 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
global step 6200 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.05 
global step 6300 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.08 
global step 6400 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
global step 6500 learning rate 0.0010 step-time 0.09 loss 1.15  perplexity 3.16 
global step 6600 learning rate 0.0010 step-time 0.09 loss 1.10  perplexity 3.00 
global step 6700 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.09 
global step 6800 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.11 
global step 6900 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.11 
global step 7000 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
global step 7100 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.06 
global step 7200 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
global step 7300 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.09 
global step 7400 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
global step 7500 learning rate 0.0010 step-time 0.09 loss 1.11  perplexity 3.04 
global step 7600 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
epoch  2 finished 
  avg train batch:  loss 1.1235 perplexity 3.0757 
  eval:  loss 1.1424 perplexity 3.1449 
global step 7700 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.09 
global step 7800 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
global step 7900 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.10 
global step 8000 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.10 
global step 8100 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.06 
global step 8200 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.09 
global step 8300 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
global step 8400 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.08 
global step 8500 learning rate 0.0010 step-time 0.09 loss 1.11  perplexity 3.05 
global step 8600 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
global step 8700 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.08 
global step 8800 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.08 
global step 8900 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.09 
global step 9000 learning rate 0.0010 step-time 0.09 loss 1.11  perplexity 3.03 
global step 9100 learning rate 0.0010 step-time 0.09 loss 1.11  perplexity 3.05 
global step 9200 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
global step 9300 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
global step 9400 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.08 
global step 9500 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.08 
global step 9600 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.08 
global step 9700 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
global step 9800 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.06 
global step 9900 learning rate 0.0010 step-time 0.09 loss 1.11  perplexity 3.04 
global step 10000 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.12 
global step 10100 learning rate 0.0010 step-time 0.09 loss 1.10  perplexity 3.01 
epoch  3 finished 
  avg train batch:  loss 1.1220 perplexity 3.0709 
  eval:  loss 1.1392 perplexity 3.1357 
global step 10200 learning rate 0.0010 step-time 0.09 loss 1.11  perplexity 3.04 
global step 10300 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.06 
global step 10400 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.05 
global step 10500 learning rate 0.0010 step-time 0.09 loss 1.11  perplexity 3.04 
global step 10600 learning rate 0.0010 step-time 0.09 loss 1.11  perplexity 3.04 
global step 10700 learning rate 0.0010 step-time 0.09 loss 1.11  perplexity 3.04 
global step 10800 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
global step 10900 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.12 
global step 11000 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.06 
global step 11100 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.10 
global step 11200 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.05 
global step 11300 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.09 
global step 11400 learning rate 0.0010 step-time 0.09 loss 1.11  perplexity 3.05 
global step 11500 learning rate 0.0010 step-time 0.09 loss 1.11  perplexity 3.05 
global step 11600 learning rate 0.0010 step-time 0.09 loss 1.11  perplexity 3.03 
global step 11700 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
global step 11800 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.09 
global step 11900 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.07 
global step 12000 learning rate 0.0010 step-time 0.09 loss 1.11  perplexity 3.04 
global step 12100 learning rate 0.0010 step-time 0.09 loss 1.15  perplexity 3.15 
global step 12200 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.12 
global step 12300 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.10 
global step 12400 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.09 
global step 12500 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.08 
global step 12600 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.08 
global step 12700 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.10 
epoch  4 finished 
  avg train batch:  loss 1.1226 perplexity 3.0729 
  eval:  loss 1.1437 perplexity 3.1493 
global step 12800 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.09 
global step 12900 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.10 
global step 13000 learning rate 0.0010 step-time 0.09 loss 1.15  perplexity 3.16 
global step 13100 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.12 
global step 13200 learning rate 0.0010 step-time 0.09 loss 1.16  perplexity 3.18 
global step 13300 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.12 
global step 13400 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.08 
global step 13500 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.11 
global step 13600 learning rate 0.0010 step-time 0.09 loss 1.13  perplexity 3.11 
global step 13700 learning rate 0.0010 step-time 0.09 loss 1.15  perplexity 3.14 
global step 13800 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.14 
global step 13900 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.14 
global step 14000 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.13 
global step 14100 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.14 
global step 14200 learning rate 0.0010 step-time 0.09 loss 1.12  perplexity 3.08 
global step 14300 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.22 
global step 14400 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.22 
global step 14500 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.14 
global step 14600 learning rate 0.0010 step-time 0.09 loss 1.16  perplexity 3.19 
global step 14700 learning rate 0.0010 step-time 0.09 loss 1.16  perplexity 3.19 
global step 14800 learning rate 0.0010 step-time 0.09 loss 1.16  perplexity 3.20 
global step 14900 learning rate 0.0010 step-time 0.09 loss 1.16  perplexity 3.20 
global step 15000 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.21 
global step 15100 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.23 
global step 15200 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.21 
epoch  5 finished 
  avg train batch:  loss 1.1494 perplexity 3.1563 
  eval:  loss 1.1741 perplexity 3.2473 
global step 15300 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.25 
global step 15400 learning rate 0.0010 step-time 0.09 loss 1.16  perplexity 3.20 
global step 15500 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.22 
global step 15600 learning rate 0.0010 step-time 0.09 loss 1.16  perplexity 3.20 
global step 15700 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.21 
global step 15800 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.24 
global step 15900 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.23 
global step 16000 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.22 
global step 16100 learning rate 0.0010 step-time 0.09 loss 1.16  perplexity 3.18 
global step 16200 learning rate 0.0010 step-time 0.09 loss 1.16  perplexity 3.18 
global step 16300 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.22 
global step 16400 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.27 
global step 16500 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.21 
global step 16600 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.21 
global step 16700 learning rate 0.0010 step-time 0.09 loss 1.16  perplexity 3.18 
global step 16800 learning rate 0.0010 step-time 0.09 loss 1.14  perplexity 3.13 
global step 16900 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.22 
global step 17000 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.24 
global step 17100 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.26 
global step 17200 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.22 
global step 17300 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.34 
global step 17400 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.25 
global step 17500 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.30 
global step 17600 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.25 
global step 17700 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.26 
epoch  6 finished 
  avg train batch:  loss 1.1720 perplexity 3.2285 
  eval:  loss 1.1862 perplexity 3.2874 
global step 17800 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.27 
global step 17900 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.27 
global step 18000 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.30 
global step 18100 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.24 
global step 18200 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.26 
global step 18300 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.34 
global step 18400 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.28 
global step 18500 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.27 
global step 18600 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.33 
global step 18700 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.22 
global step 18800 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.27 
global step 18900 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.28 
global step 19000 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.27 
global step 19100 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.25 
global step 19200 learning rate 0.0010 step-time 0.09 loss 1.16  perplexity 3.20 
global step 19300 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.29 
global step 19400 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.24 
global step 19500 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.25 
global step 19600 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.27 
global step 19700 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.27 
global step 19800 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.25 
global step 19900 learning rate 0.0010 step-time 0.09 loss 1.16  perplexity 3.20 
global step 20000 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.24 
global step 20100 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.26 
global step 20200 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.24 
global step 20300 learning rate 0.0010 step-time 0.09 loss 1.16  perplexity 3.20 
epoch  7 finished 
  avg train batch:  loss 1.1817 perplexity 3.2600 
  eval:  loss 1.1852 perplexity 3.2854 
global step 20400 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.24 
global step 20500 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.26 
global step 20600 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.23 
global step 20700 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.30 
global step 20800 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.23 
global step 20900 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.27 
global step 21000 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.24 
global step 21100 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.28 
global step 21200 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.26 
global step 21300 learning rate 0.0010 step-time 0.09 loss 1.15  perplexity 3.16 
global step 21400 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.29 
global step 21500 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.24 
global step 21600 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.26 
global step 21700 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.22 
global step 21800 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.23 
global step 21900 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 22000 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.24 
global step 22100 learning rate 0.0010 step-time 0.09 loss 1.16  perplexity 3.20 
global step 22200 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.26 
global step 22300 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.29 
global step 22400 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 22500 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.28 
global step 22600 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.25 
global step 22700 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.28 
global step 22800 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.21 
epoch  8 finished 
  avg train batch:  loss 1.1801 perplexity 3.2548 
  eval:  loss 1.1913 perplexity 3.3040 
global step 22900 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.27 
global step 23000 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 23100 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.28 
global step 23200 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.26 
global step 23300 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.29 
global step 23400 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 23500 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.30 
global step 23600 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.33 
global step 23700 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.37 
global step 23800 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 23900 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 24000 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.32 
global step 24100 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.45 
global step 24200 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.41 
global step 24300 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 24400 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 24500 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 24600 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.33 
global step 24700 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 24800 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.32 
global step 24900 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 25000 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.32 
global step 25100 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.33 
global step 25200 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.34 
global step 25300 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.32 
global step 25400 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.29 
epoch  9 finished 
  avg train batch:  loss 1.2027 perplexity 3.3289 
  eval:  loss 1.2019 perplexity 3.3400 
global step 25500 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.32 
global step 25600 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.26 
global step 25700 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.24 
global step 25800 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 25900 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.29 
global step 26000 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.32 
global step 26100 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.32 
global step 26200 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.28 
global step 26300 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 26400 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.27 
global step 26500 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.34 
global step 26600 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.32 
global step 26700 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.30 
global step 26800 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 26900 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.30 
global step 27000 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 27100 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.32 
global step 27200 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.32 
global step 27300 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.28 
global step 27400 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.27 
global step 27500 learning rate 0.0010 step-time 0.09 loss 1.17  perplexity 3.22 
global step 27600 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 27700 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.26 
global step 27800 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.33 
global step 27900 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.29 
epoch  10 finished 
  avg train batch:  loss 1.1927 perplexity 3.2960 
  eval:  loss 1.2028 perplexity 3.3432 
global step 28000 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 28100 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 28200 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.37 
global step 28300 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 28400 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.40 
global step 28500 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 28600 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.32 
global step 28700 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.37 
global step 28800 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.34 
global step 28900 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.29 
global step 29000 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 29100 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 29200 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.28 
global step 29300 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 29400 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 29500 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.34 
global step 29600 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 29700 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.26 
global step 29800 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 29900 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 30000 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.42 
global step 30100 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.39 
global step 30200 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.32 
global step 30300 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.37 
global step 30400 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.39 
global step 30500 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.34 
epoch  11 finished 
  avg train batch:  loss 1.2063 perplexity 3.3412 
  eval:  loss 1.2226 perplexity 3.4109 
global step 30600 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.37 
global step 30700 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 30800 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.30 
global step 30900 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.30 
global step 31000 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.33 
global step 31100 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 31200 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.33 
global step 31300 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.33 
global step 31400 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.28 
global step 31500 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.29 
global step 31600 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.25 
global step 31700 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.27 
global step 31800 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.33 
global step 31900 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.28 
global step 32000 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 32100 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 32200 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.32 
global step 32300 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.30 
global step 32400 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 32500 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 32600 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 32700 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.32 
global step 32800 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.29 
global step 32900 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.34 
global step 33000 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
epoch  12 finished 
  avg train batch:  loss 1.2010 perplexity 3.3234 
  eval:  loss 1.2159 perplexity 3.3875 
global step 33100 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.33 
global step 33200 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.30 
global step 33300 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.39 
global step 33400 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 33500 learning rate 0.0010 step-time 0.09 loss 1.19  perplexity 3.30 
global step 33600 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.34 
global step 33700 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.37 
global step 33800 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.40 
global step 33900 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.34 
global step 34000 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.45 
global step 34100 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.39 
global step 34200 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 34300 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 34400 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.37 
global step 34500 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.34 
global step 34600 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 34700 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.40 
global step 34800 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.41 
global step 34900 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.34 
global step 35000 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.37 
global step 35100 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.39 
global step 35200 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 35300 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.39 
global step 35400 learning rate 0.0010 step-time 0.09 loss 1.25  perplexity 3.48 
global step 35500 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.40 
epoch  13 finished 
  avg train batch:  loss 1.2148 perplexity 3.3698 
  eval:  loss 1.2263 perplexity 3.4241 
global step 35600 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 35700 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 35800 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.39 
global step 35900 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.41 
global step 36000 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.44 
global step 36100 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.42 
global step 36200 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.37 
global step 36300 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.40 
global step 36400 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.37 
global step 36500 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 36600 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 36700 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.44 
global step 36800 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 36900 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.43 
global step 37000 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 37100 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 37200 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 37300 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 37400 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 37500 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.43 
global step 37600 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.39 
global step 37700 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 37800 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 37900 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.37 
global step 38000 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 38100 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.41 
epoch  14 finished 
  avg train batch:  loss 1.2196 perplexity 3.3859 
  eval:  loss 1.2310 perplexity 3.4381 
global step 38200 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.39 
global step 38300 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.42 
global step 38400 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.37 
global step 38500 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.45 
global step 38600 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.45 
global step 38700 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.46 
global step 38800 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.43 
global step 38900 learning rate 0.0010 step-time 0.09 loss 1.25  perplexity 3.48 
global step 39000 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.44 
global step 39100 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.45 
global step 39200 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.42 
global step 39300 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.45 
global step 39400 learning rate 0.0010 step-time 0.09 loss 1.25  perplexity 3.49 
global step 39500 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.42 
global step 39600 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.42 
global step 39700 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 39800 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.45 
global step 39900 learning rate 0.0010 step-time 0.09 loss 1.25  perplexity 3.49 
global step 40000 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.42 
global step 40100 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.41 
global step 40200 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.42 
global step 40300 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 40400 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 40500 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.43 
global step 40600 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.40 
epoch  15 finished 
  avg train batch:  loss 1.2311 perplexity 3.4249 
  eval:  loss 1.2274 perplexity 3.4277 
global step 40700 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.33 
global step 40800 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 40900 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 41000 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.41 
global step 41100 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 41200 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.33 
global step 41300 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 41400 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 41500 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.41 
global step 41600 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.41 
global step 41700 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.37 
global step 41800 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 41900 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 42000 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 42100 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.40 
global step 42200 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.34 
global step 42300 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 42400 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 42500 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.39 
global step 42600 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.41 
global step 42700 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.39 
global step 42800 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.39 
global step 42900 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.42 
global step 43000 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.30 
global step 43100 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.40 
global step 43200 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.42 
epoch  16 finished 
  avg train batch:  loss 1.2175 perplexity 3.3788 
  eval:  loss 1.2278 perplexity 3.4286 
global step 43300 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 43400 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 43500 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.33 
global step 43600 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 43700 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 43800 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 43900 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.39 
global step 44000 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.40 
global step 44100 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.33 
global step 44200 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.41 
global step 44300 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.43 
global step 44400 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 44500 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 44600 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 44700 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 44800 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.41 
global step 44900 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.41 
global step 45000 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.42 
global step 45100 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 45200 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.37 
global step 45300 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.40 
global step 45400 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.42 
global step 45500 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 45600 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.45 
global step 45700 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.45 
epoch  17 finished 
  avg train batch:  loss 1.2188 perplexity 3.3833 
  eval:  loss 1.2385 perplexity 3.4648 
global step 45800 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.46 
global step 45900 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.45 
global step 46000 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.45 
global step 46100 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.42 
global step 46200 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 46300 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.32 
global step 46400 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.41 
global step 46500 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 46600 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 46700 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 46800 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 46900 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.34 
global step 47000 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 47100 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.32 
global step 47200 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.35 
global step 47300 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.34 
global step 47400 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.33 
global step 47500 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.37 
global step 47600 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.34 
global step 47700 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.33 
global step 47800 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 47900 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 48000 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.40 
global step 48100 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.40 
global step 48200 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
epoch  18 finished 
  avg train batch:  loss 1.2159 perplexity 3.3733 
  eval:  loss 1.2358 perplexity 3.4567 
global step 48300 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.42 
global step 48400 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.45 
global step 48500 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.41 
global step 48600 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.40 
global step 48700 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.44 
global step 48800 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.37 
global step 48900 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.42 
global step 49000 learning rate 0.0010 step-time 0.09 loss 1.24  perplexity 3.44 
global step 49100 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.42 
global step 49200 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.41 
global step 49300 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.39 
global step 49400 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.40 
global step 49500 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.42 
global step 49600 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.40 
global step 49700 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.43 
global step 49800 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.43 
global step 49900 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.37 
global step 50000 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
global step 50100 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.34 
global step 50200 learning rate 0.0010 step-time 0.09 loss 1.21  perplexity 3.36 
global step 50300 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.40 
global step 50400 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.32 
global step 50500 learning rate 0.0010 step-time 0.09 loss 1.23  perplexity 3.41 
global step 50600 learning rate 0.0010 step-time 0.09 loss 1.20  perplexity 3.31 
global step 50700 learning rate 0.0010 step-time 0.09 loss 1.18  perplexity 3.27 
global step 50800 learning rate 0.0010 step-time 0.09 loss 1.22  perplexity 3.38 
epoch  19 finished 
  avg train batch:  loss 1.2209 perplexity 3.3903 
  eval:  loss 1.2510 perplexity 3.5094 
Reached the maximum number of epochs 20  stop trainig 
best training loss 3.0709 best validation 3.1357 
		test: loss 1.2537  perplexity 3.5186 